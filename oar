#!/usr/bin/env python3

# Import system modules
import os,sys
import struct 
from subprocess import Popen,check_output
from struct import pack,unpack
from ctypes import *
import json
import operator
import argparse

os.environ['OMP_TOOL_LIBRARIES'] = 'lib/liboar.so'
test_nthreads=[4,8,16,32,64,128]
program_name="bin/hello"

class readabledata(Structure):
        _fields_ = [ ('id', c_int),
                     ('workers', c_int),
                     ('source', c_wchar_p),
                     ('time', c_float)] 

class perfdata(Structure):
	_fields_ = [ ('id', c_int),
                     ('workers', c_int),
		     ('codeptr', c_void_p),
		     ('begin',c_long),
		     ('end',c_long)]

def parse_arguments():
  parser = argparse.ArgumentParser(description='OAR: OpenMP Adaptive Runtime')
  parser.add_argument('mode',choices=['collect','run'],help='choose between collect and run modes')
  parser.add_argument('--aggregate',default=False,action='store_true',help='aggregate performance of multiple invocation of the same OpenMP region')
  parser.add_argument('--nworkers_range',metavar='nworkers_range',type=str)
  parser.add_argument('--nworkers_list',metavar='nworkers_list',type=str)
  args = parser.parse_args()
  return args

def perf2readable(perfdata_parallel,readabledata_parallel,aggregate):
  if not aggregate: # each parallel call is treated as unique even if it relates to the same loop
    for idx in range(len(perfdata_parallel)):
      id = perfdata_parallel[idx].id
      workers = perfdata_parallel[idx].workers
      source = str(check_output("addr2line -e " + program_name + " " + hex(perfdata_parallel[idx].codeptr), shell=True),'utf-8').rstrip("\n")
      time = (perfdata_parallel[idx].end-perfdata_parallel[idx].begin)/1000000.0 # converting to seconds
      readabledata_parallel.append(readabledata(id,workers,source,time))
  else: # performance of the same loop will be averaged over all executions in the code
    count = len(perfdata_parallel)*[0]
    for idx in range(len(perfdata_parallel)):
      id = perfdata_parallel[idx].id
      workers = perfdata_parallel[idx].workers
      source = str(check_output("addr2line -e " + program_name + " " + hex(perfdata_parallel[idx].codeptr), shell=True),'utf-8').rstrip("\n")
      time = (perfdata_parallel[idx].end-perfdata_parallel[idx].begin)/1000000.0 # converting to seconds
      new_instance = True
      for idx2 in range(len(readabledata_parallel)):
        if (source == readabledata_parallel[idx2].source and workers == readabledata_parallel[idx2].workers) :
          new_instance = False
          count[idx2]=count[idx2]+1
          readabledata_parallel[idx2].time = readabledata_parallel[idx2].time + time
      if new_instance == True:
        count[idx]=1
        readabledata_parallel.append(readabledata(id,workers,source,time))
    for idx2 in range(len(readabledata_parallel)):
      readabledata_parallel[idx2].time = readabledata_parallel[idx2].time / count[idx2]       

def print_json(readabledata_parallel):
  idx=0
  fp=open('hello.json','w')
  rid = int(len(readabledata_parallel)/len(test_nthreads))
  for region in range(int(rid)):
    timing = {}
    id = readabledata_parallel[idx].id
    source = readabledata_parallel[idx].source
    for workers in range(len(test_nthreads)):
      timing[readabledata_parallel[idx].workers]=readabledata_parallel[idx].time
      idx=idx+1
    json_dump = {"id":id, "source":source, "performance":[{'workers':key,'time':value} for key,value in timing.items()]}
    json.dump(json_dump, fp, indent=2, separators=(',', ': '))

def create_nworkers_list(args):
  nworkers_list = []
  if args.nworkers_range: # range provided
    arg_list = args.nworkers_range.split(":")
    arg_list = [int(x) for x in arg_list]
    n = arg_list[0]
    while n <= arg_list[1]:
      nworkers_list.append(n)
      n = n * arg_list[2]
  elif args.nworkers_list: # list provided 
    arg_list = args.nworkers_list.split(",")
    arg_list = [int(x) for x in arg_list]
    nworkers_list = arg_list
  else: # default is read OMP_NUM_THREADS and decrease
    nworkers_list=[4,8,16,32,64,128]
  return nworkers_list

def collect(args):
  nworkers_list = create_nworkers_list(args)
  perfdata_parallel = []
  readabledata_parallel = []
  for nthreads in nworkers_list:
    p=Popen(program_name)
    fifo = "./pipe"
    os.mkfifo(fifo,0o666)
    fd=os.open(fifo,os.O_WRONLY)
    os.write(fd,pack('i',nthreads))
    os.close(fd)
    fd=os.open(fifo,os.O_RDONLY)
    rid=int.from_bytes(os.read(fd,4),byteorder="little")
    for idx in range(rid):
      perfdata_binary=os.read(fd,sizeof(perfdata))
      id, workers, codeptr, begin, end = struct.unpack('iiPll',perfdata_binary)
      perfdata_parallel.append(perfdata(id,workers,codeptr,begin,end))
    os.close(fd)
    p.communicate()
    os.remove(fifo)
  perfdata_parallel.sort(key=operator.attrgetter('id'))
  perf2readable(perfdata_parallel,readabledata_parallel,args.aggregate)
  print_json(readabledata_parallel)

def main():
  args=parse_arguments()
  print(args.mode)
  if args.mode == 'collect': collect(args)

if __name__ == "__main__":
    main()
